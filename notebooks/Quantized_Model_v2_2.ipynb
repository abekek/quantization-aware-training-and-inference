{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abekek/quantization-aware-training-and-inference/blob/main/Quantized_Model_v2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if running on collaboratory set = True\n",
        "collaboratory = True\n",
        "\n",
        "if collaboratory:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else: \n",
        "    print('Running on local systems, if running on collaboratory please change above')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGjexShWGrao",
        "outputId": "f36dc3c5-b2c6-4195-ffbe-ce2703fb3cb2"
      },
      "id": "KGjexShWGrao",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# changes directory to your main google drive folder\n",
        "%cd drive/My\\ Drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65MnmUe1G0Dn",
        "outputId": "afecef76-3256-4e9d-c0df-edace406e6c1"
      },
      "id": "65MnmUe1G0Dn",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1ea23a19",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea23a19",
        "outputId": "8a2d8087-5e07-475f-9b4f-9368726b9795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Quantized-SHO-Fitting\n"
          ]
        }
      ],
      "source": [
        "# Checks if the directory exists\n",
        "import os\n",
        "if os.path.exists(\"./Quantized-SHO-Fitting\"):\n",
        "    %cd Quantized-SHO-Fitting\n",
        "else:\n",
        "    %mkdir Quantized-SHO-Fitting\n",
        "    %cd Quantized-SHO-Fitting\n",
        "    !git clone https://github.com/abekek/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "08d2c164",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08d2c164",
        "outputId": "8bccf3e0-0df6-4044-a851-97c57e6de740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Quantized-SHO-Fitting/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting\n"
          ]
        }
      ],
      "source": [
        "# moves to the right directory\n",
        "%cd Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aca15c64",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aca15c64",
        "outputId": "fb263e7b-1a00-410b-8c49-0912b9374017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# checks if the directory is up to date\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f682eb2",
      "metadata": {
        "id": "1f682eb2"
      },
      "source": [
        "## Installation & Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "542d089d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "542d089d",
        "outputId": "5a1452f2-82de-4856-aac8-2e77c7c8918e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: atomicwrites==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Collecting attrs==20.3.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting autopep8==1.5.4\n",
            "  Downloading autopep8-1.5.4.tar.gz (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting certifi==2020.12.5\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 20.2 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 34.1 MB/s \n",
            "\u001b[?25hCollecting colorama==0.4.4\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting coverage==5.3.1\n",
            "  Downloading coverage-5.3.1-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.6.2)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: iniconfig==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.1.1)\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 51.5 MB/s \n",
            "\u001b[?25hCollecting packaging==20.8\n",
            "  Downloading packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
            "Collecting Pillow==8.1.0\n",
            "  Downloading Pillow-8.1.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 61.3 MB/s \n",
            "\u001b[?25hCollecting pipreqs==0.4.10\n",
            "  Downloading pipreqs-0.4.10-py2.py3-none-any.whl (25 kB)\n",
            "Collecting pluggy==0.13.1\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting py==1.10.0\n",
            "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting pycodestyle==2.6.0\n",
            "  Downloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 372 kB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting pytest==6.2.1\n",
            "  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 69.4 MB/s \n",
            "\u001b[?25hCollecting pytest-cov==2.11.1\n",
            "  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting toml==0.10.2\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions==3.7.4.3\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting urllib3==1.26.2\n",
            "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 95.6 MB/s \n",
            "\u001b[?25hCollecting yarg==0.1.9\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Collecting numpy_groupies==0.9.7\n",
            "  Downloading numpy_groupies-0.9.7.tar.gz (22 kB)\n",
            "Requirement already satisfied: dask==2.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (2.12.0)\n",
            "Requirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (1.1.0)\n",
            "Collecting tensorflow==2.4.1\n",
            "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3 MB 14 kB/s \n",
            "\u001b[?25hCollecting sidpy==0.0.5\n",
            "  Downloading sidpy-0.0.5-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.6 MB/s \n",
            "\u001b[?25hCollecting ipywidgets==7.6.3\n",
            "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 78.2 MB/s \n",
            "\u001b[?25hCollecting scikit_image==0.16.2\n",
            "  Downloading scikit_image-0.16.2-cp37-cp37m-manylinux1_x86_64.whl (26.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.5 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting joblib==1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 75.1 MB/s \n",
            "\u001b[?25hCollecting pip==19.3.1\n",
            "  Downloading pip-19.3.1-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 26.0 MB/s \n",
            "\u001b[?25hCollecting ipython==7.22.0\n",
            "  Downloading ipython-7.22.0-py3-none-any.whl (785 kB)\n",
            "\u001b[K     |████████████████████████████████| 785 kB 58.3 MB/s \n",
            "\u001b[?25hCollecting pyqtgraph==0.12.1\n",
            "  Downloading pyqtgraph-0.12.1-py3-none-any.whl (939 kB)\n",
            "\u001b[K     |████████████████████████████████| 939 kB 21.6 MB/s \n",
            "\u001b[?25hCollecting pyUSID==0.0.10\n",
            "  Downloading pyUSID-0.0.10-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting scikit_learn==0.24.1\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting sphinx_rtd_theme==0.5.2\n",
            "  Downloading sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 82.0 MB/s \n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting BGlib==0.0.3\n",
            "  Downloading BGlib-0.0.3-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 86.7 MB/s \n",
            "\u001b[?25hCollecting pycroscopy==0.60.7\n",
            "  Downloading pycroscopy-0.60.7-py2.py3-none-any.whl (354 kB)\n",
            "\u001b[K     |████████████████████████████████| 354 kB 77.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (3.2.2)\n",
            "Requirement already satisfied: moviepy==0.2.3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 45)) (0.2.3.5)\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 26.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 47)) (1.4.1)\n",
            "Collecting pygame==2.0.1\n",
            "  Downloading pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 24.0 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0.dev20210415+cu101 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0.dev20210415+cu101\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installs all of the requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b8488bf",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4b8488bf",
        "outputId": "137bb3a8-103f-4a04-cb2b-298ad5a3915b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycroscopy==0.60.7\n",
            "  Using cached pycroscopy-0.60.7-py2.py3-none-any.whl (354 kB)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.21.6)\n",
            "Requirement already satisfied: scikit-image>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (0.18.3)\n",
            "Requirement already satisfied: xlrd>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.4.1)\n",
            "Requirement already satisfied: ipywidgets>=5.2.2 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (7.7.0)\n",
            "Collecting numpy-groupies==0.9.7\n",
            "  Using cached numpy_groupies-0.9.7.tar.gz (22 kB)\n",
            "Collecting pyUSID>=0.0.8\n",
            "  Using cached pyUSID-0.0.10-py2.py3-none-any.whl (66 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (7.1.2)\n",
            "Collecting gwyfile\n",
            "  Downloading gwyfile-0.2.0-py2.py3-none-any.whl (8.9 kB)\n",
            "Collecting igor\n",
            "  Downloading igor-0.3.tar.gz (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 226 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.15.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (5.4.8)\n",
            "Collecting ipython>=6.0\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting sidpy>=0.0.1\n",
            "  Downloading sidpy-0.0.9-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.6.0->pycroscopy==0.60.7) (1.5.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.1.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.18.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (2.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.1.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.4.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.10.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.3.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=6.0->pycroscopy==0.60.7) (0.8.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->pycroscopy==0.60.7) (4.1.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.11.4)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (21.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=6.0->pycroscopy==0.60.7) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0->pycroscopy==0.60.7) (0.2.5)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (0.11.2)\n",
            "Requirement already satisfied: dask>=0.10 in /usr/local/lib/python3.7/dist-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (2.12.0)\n",
            "Collecting cytoolz\n",
            "  Downloading cytoolz-0.11.2.tar.gz (481 kB)\n",
            "\u001b[K     |████████████████████████████████| 481 kB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2.6.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->pycroscopy==0.60.7) (3.1.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from sidpy>=0.0.1->pycroscopy==0.60.7) (0.0)\n",
            "Collecting ipyfilechooser>=0.0.6\n",
            "  Downloading ipyfilechooser-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting distributed>=2.0.0psutil\n",
            "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (21.3)\n",
            "Collecting dask>=0.10\n",
            "  Downloading dask-2022.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 75.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.7.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.2.0)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (7.1.2)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.0.4)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (3.13)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 97.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 63.0 MB/s \n",
            "\u001b[?25hCollecting locket\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (23.1.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.5.1)\n",
            "Building wheels for collected packages: numpy-groupies, cytoolz, igor\n",
            "  Building wheel for numpy-groupies (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpy-groupies: filename=numpy_groupies-0.9.7-py3-none-any.whl size=21315 sha256=b8611d8b1c76d793d5771b0ef8599eefdb5c8e13aa8fcce533e0f183af36002a\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/05/e1/250ebec6656f2a0bc1141d5c185876dcd74e7e47740613c9d6\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.11.2-cp37-cp37m-linux_x86_64.whl size=1236744 sha256=aeb49f74fe241028309f6673a71095cf9451c518c916265c00b1b2110097e8dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/70/71/ca13ea3d36ccd0b3d0ec7d7a4ca67522048d695b556bba4f59\n",
            "  Building wheel for igor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for igor: filename=igor-0.3-py3-none-any.whl size=52116 sha256=0e4cfa3ee5cb9640c71c8a61f8a9bdce8084a5672f4c8cd5a5b13b84efbe45db\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/27/d2/31051f074caeea50e0d11890508c40e9456af990a9350d0fb6\n",
            "Successfully built numpy-groupies cytoolz igor\n",
            "Installing collected packages: prompt-toolkit, ipython, locket, pyyaml, partd, fsspec, cloudpickle, dask, ipyfilechooser, distributed, cytoolz, sidpy, pyUSID, numpy-groupies, igor, gwyfile, pycroscopy\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-2.1.0 cytoolz-0.11.2 dask-2022.2.0 distributed-2022.2.0 fsspec-2022.5.0 gwyfile-0.2.0 igor-0.3 ipyfilechooser-0.6.0 ipython-7.34.0 locket-1.0.0 numpy-groupies-0.9.7 partd-1.2.0 prompt-toolkit-3.0.29 pyUSID-0.0.10 pycroscopy-0.60.7 pyyaml-6.0 sidpy-0.0.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting numpy>=1.7\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 2.1 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.6 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# installing PyTorch's Nightly version\n",
        "!pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html -U\n",
        "\n",
        "!pip install pycroscopy==0.60.7\n",
        "\n",
        "if os.path.exists(\"./BGlib\"):\n",
        "    pass\n",
        "else:\n",
        "    !git clone https://github.com/pycroscopy/BGlib.git\n",
        "    %cd BGlib/\n",
        "    !git tag -l\n",
        "    !git checkout 0.0.3\n",
        "    !git branch -D master\n",
        "    !git checkout -b master\n",
        "    %cd ..\n",
        "\n",
        "# downgrading the h5py version\n",
        "!pip install 'h5py==2.10.0' --force-reinstal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c5448f6",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c5448f6",
        "outputId": "d7ca7f95-7f6b-4dc7-e447-e3468bde8f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hls4ml\n",
            "  Downloading hls4ml-0.6.0-py3-none-any.whl (295 kB)\n",
            "\u001b[K     |████████████████████████████████| 295 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hls4ml) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hls4ml) (1.16.0)\n",
            "Collecting onnx>=1.4.0\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 29.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from hls4ml) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from hls4ml) (6.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.4.0->hls4ml) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.4.0->hls4ml) (4.1.1)\n",
            "Installing collected packages: onnx, hls4ml\n",
            "Successfully installed hls4ml-0.6.0 onnx-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install hls4ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f2295d0c",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2295d0c",
        "outputId": "a53c0ab2-aa97-440e-906c-ecbb89145181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.6)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c54749b4",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c54749b4",
        "outputId": "92dc7a62-10f6-40b2-dbc1-fbe0c4c71ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy) (1.21.6)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (2.4.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f8b88ba0",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8b88ba0",
        "outputId": "744f03d8-13ab-4f3f-ea89-d4747616fe52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting qkeras\n",
            "  Downloading QKeras-0.9.0-py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from qkeras) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from qkeras) (1.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from qkeras) (57.4.0)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 20.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from qkeras) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.7/dist-packages (from qkeras) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from qkeras) (1.0.2)\n",
            "Collecting keras-tuner>=1.0.1\n",
            "  Downloading keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting pyparser\n",
            "  Downloading pyparser-1.0.tar.gz (4.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.1->qkeras) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.1->qkeras) (21.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.1->qkeras) (2.8.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.1->qkeras) (7.34.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->qkeras) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->qkeras) (3.1.0)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.2.1->qkeras) (1.16.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.2.1->qkeras) (0.1.7)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (2.6.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.1.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.18.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (3.0.29)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython->keras-tuner>=1.0.1->qkeras) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython->keras-tuner>=1.0.1->qkeras) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner>=1.0.1->qkeras) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner>=1.0.1->qkeras) (3.0.9)\n",
            "Collecting parse==1.6.5\n",
            "  Downloading parse-1.6.5.tar.gz (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.1->qkeras) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.1->qkeras) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.1->qkeras) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.1->qkeras) (2.10)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.46.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.1->qkeras) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner>=1.0.1->qkeras) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner>=1.0.1->qkeras) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner>=1.0.1->qkeras) (4.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.1->qkeras) (3.2.0)\n",
            "Building wheels for collected packages: pyparser, parse\n",
            "  Building wheel for pyparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyparser: filename=pyparser-1.0-py3-none-any.whl size=4943 sha256=573b94369a98ded6901450ad949468116aaf64b8e24619c106d2816fba1e49b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/80/fe/49e0cb63aba370d3ef38e733a2266c90a4d837921664320003\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.6.5-py3-none-any.whl size=18176 sha256=c6160ceefbf2403958d216f99d6e53c13271f49876f649808f722f05ba953c5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/d2/3e/3df86c4fd6ebac1348fbbda0a551e28cacf7301969935732dd\n",
            "Successfully built pyparser parse\n",
            "Installing collected packages: parse, kt-legacy, tensorflow-model-optimization, pyparser, keras-tuner, qkeras\n",
            "Successfully installed keras-tuner-1.1.2 kt-legacy-1.0.4 parse-1.6.5 pyparser-1.0 qkeras-0.9.0 tensorflow-model-optimization-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install qkeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d8bbe41f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8bbe41f",
        "outputId": "642f6c28-f08e-4555-8dfe-5a969c03f3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hls4ml[profiling] in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hls4ml[profiling]) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hls4ml[profiling]) (1.21.6)\n",
            "Requirement already satisfied: onnx>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from hls4ml[profiling]) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from hls4ml[profiling]) (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from hls4ml[profiling]) (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from hls4ml[profiling]) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from hls4ml[profiling]) (0.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from hls4ml[profiling]) (1.3.5)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.4.0->hls4ml[profiling]) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.4.0->hls4ml[profiling]) (4.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->hls4ml[profiling]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->hls4ml[profiling]) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->hls4ml[profiling]) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->hls4ml[profiling]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->hls4ml[profiling]) (2022.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->hls4ml[profiling]) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install hls4ml[profiling]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7d22a181",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "7d22a181",
        "outputId": "95981273-d070-46a8-9531-b3d4c31ec4e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b729088/45929032 bytes (1.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3620864/45929032 bytes (7.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7143424/45929032 bytes (15.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10395648/45929032 bytes (22.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13426688/45929032 bytes (29.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17104896/45929032 bytes (37.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20856832/45929032 bytes (45.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24313856/45929032 bytes (52.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27860992/45929032 bytes (60.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31227904/45929032 bytes (68.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34684928/45929032 bytes (75.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37707776/45929032 bytes (82.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41435136/45929032 bytes (90.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44638208/45929032 bytes (97.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import multiprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numba import jit\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import argparse\n",
        "import seaborn as sns\n",
        "from scipy.signal import resample\n",
        "from scipy import fftpack\n",
        "from scipy import io\n",
        "from scipy import special\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.layers import (Attention, Dense, Conv1D, Convolution2D, \n",
        "                                     GRU, LSTM, Bidirectional, TimeDistributed,\n",
        "                                     Dropout, Flatten, LayerNormalization, \n",
        "                                     RepeatVector, Reshape, MaxPooling1D, \n",
        "                                     UpSampling1D, BatchNormalization, Activation)\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from multiprocessing import Pool, Process\n",
        "import multiprocessing as mp\n",
        "from moviepy.editor import *\n",
        "import glob\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import gc\n",
        "import sidpy\n",
        "from BGlib.BGlib import be as belib\n",
        " \n",
        "# set up notebook to show plots within the notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "\n",
        "# Import necessary libraries:\n",
        "# General utilities:\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Computation:\n",
        "import numpy as np\n",
        "import h5py\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization:\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "# Finally, pycroscopy itself\n",
        "sys.path.append('../../../')\n",
        "import pyUSID as usid\n",
        "from codes.util.preprocessing_global_standard_scaler import global_standard_scaler\n",
        "from sidpy.hdf.hdf_utils import write_simple_attrs, get_attr\n",
        "from pyUSID.io.hdf_utils import create_results_group, write_main_dataset, write_reduced_anc_dsets, create_empty_dataset, reshape_to_n_dims, get_auxiliary_datasets\n",
        "from pyUSID.io.usi_data import USIDataset\n",
        "from pyUSID.io import Dimension\n",
        "\n",
        "from codes.util.file import print_tree\n",
        "from codes.util.core import SHO_fit_func_torch, loop_fitting_function, loop_fitting_function_tf, computeDotProducts, normOfVar, fit_loop_function, computeTime, conventional_fit_loop_function\n",
        "from codes.viz.plot import plot_best_worst_SHO, make_movie, plot_best_worst_loops, plot_reconstruction_comparison_SHO, plot_reconstruction_comparison_loops\n",
        "from codes.util.postprocessing import transform_params, convert_real_imag\n",
        "from codes.util.preprocessing_global_scaler import global_scaler\n",
        "from codes.processing.filters import range_filter, clean_interpolate, interpolate_missing_points\n",
        "from codes.algorithm.TRPCGOptimizerv2 import TRPCGOptimizerv2\n",
        "from codes.algorithm.AdaHessian import AdaHessian\n",
        "\n",
        "import numpy.lib.recfunctions as rfn\n",
        "\n",
        "import hls4ml\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Activation, MaxPool1D, AvgPool1D, Flatten, Dense\n",
        "from tensorflow.nn import selu\n",
        "from qkeras import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f8d4d8a",
      "metadata": {
        "id": "6f8d4d8a"
      },
      "source": [
        "## Setting defaults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "89e638df",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89e638df",
        "outputId": "2d535cd7-ec8e-4d2e-aeae-8865a9ef7d8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# shows the number of CPU cores\n",
        "multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c42f1c68",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c42f1c68",
        "outputId": "ddb79d13-729e-4366-ef36-5fac93930812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 27 01:50:37 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# shows the GPU that is available and the resources\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "10bf5ee1",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10bf5ee1",
        "outputId": "ef7c1def-c59d-4240-fe10-ef792abe7b9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4a395ecd",
      "metadata": {
        "scrolled": false,
        "id": "4a395ecd"
      },
      "outputs": [],
      "source": [
        "# fixes the random seed for reproducible training\n",
        "torch.set_default_dtype(torch.float32)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a007a1e6",
      "metadata": {
        "scrolled": false,
        "id": "a007a1e6"
      },
      "outputs": [],
      "source": [
        "# resetting default seaborn style\n",
        "sns.reset_orig()\n",
        "\n",
        "# setting default plotting params\n",
        "plt.rcParams['image.cmap'] = 'magma'\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.direction'] = 'in'\n",
        "plt.rcParams['ytick.direction'] = 'in'\n",
        "plt.rcParams['xtick.top'] = True\n",
        "plt.rcParams['ytick.right'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eccef33",
      "metadata": {
        "id": "3eccef33"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b8d86e96",
      "metadata": {
        "scrolled": false,
        "id": "b8d86e96"
      },
      "outputs": [],
      "source": [
        "# gdown.download('https://drive.google.com/uc?export=download&id=1Q2Qo_1VGlCsVOTjQpZlE5tjoIV1etVe2', 'data_file_copy1.h5', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8823161d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8823161d",
        "outputId": "ee412c1e-9486-4912-85fc-cfda81c34199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "├ Measurement_000\n",
            "  ---------------\n",
            "  ├ Channel_000\n",
            "    -----------\n",
            "    ├ Bin_FFT\n",
            "    ├ Bin_Frequencies\n",
            "    ├ Bin_Indices\n",
            "    ├ Bin_Step\n",
            "    ├ Bin_Wfm_Type\n",
            "    ├ Excitation_Waveform\n",
            "    ├ Noise_Floor\n",
            "    ├ Position_Indices\n",
            "    ├ Position_Values\n",
            "    ├ Raw_Data\n",
            "    ├ Raw_Data-SHO_Fit_000\n",
            "      --------------------\n",
            "      ├ Fit\n",
            "      ├ Guess\n",
            "      ├ Spectroscopic_Indices\n",
            "      ├ Spectroscopic_Values\n",
            "      ├ completed_fit_positions\n",
            "      ├ completed_guess_positions\n",
            "    ├ Spatially_Averaged_Plot_Group_000\n",
            "      ---------------------------------\n",
            "      ├ Bin_Frequencies\n",
            "      ├ Max_Response\n",
            "      ├ Mean_Spectrogram\n",
            "      ├ Min_Response\n",
            "      ├ Spectroscopic_Parameter\n",
            "      ├ Step_Averaged_Response\n",
            "    ├ Spatially_Averaged_Plot_Group_001\n",
            "      ---------------------------------\n",
            "      ├ Bin_Frequencies\n",
            "      ├ Max_Response\n",
            "      ├ Mean_Spectrogram\n",
            "      ├ Min_Response\n",
            "      ├ Spectroscopic_Parameter\n",
            "      ├ Step_Averaged_Response\n",
            "    ├ Spectroscopic_Indices\n",
            "    ├ Spectroscopic_Values\n",
            "    ├ UDVS\n",
            "    ├ UDVS_Indices\n"
          ]
        }
      ],
      "source": [
        "# Opens the translated file\n",
        "h5_f = h5py.File('./data_file.h5', 'r+')\n",
        "\n",
        "#Inspects the h5 file\n",
        "usid.hdf_utils.print_tree(h5_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b43f7d4a",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b43f7d4a",
        "outputId": "2270b88e-a40e-4519-ce5d-1f5fd8efc9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets and datagroups within the file:\n",
            "------------------------------------\n",
            "/\n",
            "/Measurement_000\n",
            "/Measurement_000/Channel_000\n",
            "/Measurement_000/Channel_000/Bin_FFT\n",
            "/Measurement_000/Channel_000/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Bin_Indices\n",
            "/Measurement_000/Channel_000/Bin_Step\n",
            "/Measurement_000/Channel_000/Bin_Wfm_Type\n",
            "/Measurement_000/Channel_000/Excitation_Waveform\n",
            "/Measurement_000/Channel_000/Noise_Floor\n",
            "/Measurement_000/Channel_000/Position_Indices\n",
            "/Measurement_000/Channel_000/Position_Values\n",
            "/Measurement_000/Channel_000/Raw_Data\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Fit\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Guess\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Indices\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Values\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/completed_fit_positions\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/completed_guess_positions\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Max_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Mean_Spectrogram\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Min_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Spectroscopic_Parameter\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Step_Averaged_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Max_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Mean_Spectrogram\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Min_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Spectroscopic_Parameter\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Step_Averaged_Response\n",
            "/Measurement_000/Channel_000/Spectroscopic_Indices\n",
            "/Measurement_000/Channel_000/Spectroscopic_Values\n",
            "/Measurement_000/Channel_000/UDVS\n",
            "/Measurement_000/Channel_000/UDVS_Indices\n",
            "\n",
            "The main dataset:\n",
            "------------------------------------\n",
            "<HDF5 file \"data_file.h5\" (mode r+)>\n",
            "\n",
            "The ancillary datasets:\n",
            "------------------------------------\n",
            "<HDF5 dataset \"Position_Indices\": shape (3600, 2), type \"<u4\">\n",
            "<HDF5 dataset \"Position_Values\": shape (3600, 2), type \"<f4\">\n",
            "<HDF5 dataset \"Spectroscopic_Indices\": shape (4, 63360), type \"<u4\">\n",
            "<HDF5 dataset \"Spectroscopic_Values\": shape (4, 63360), type \"<f4\">\n",
            "\n",
            "Metadata or attributes in a datagroup\n",
            "------------------------------------\n",
            "BE_actual_duration_[s] : 0.004\n",
            "BE_amplitude_[V] : 1\n",
            "BE_auto_smoothing : auto smoothing on\n",
            "BE_band_edge_smoothing_[s] : 4832.1\n",
            "BE_band_edge_trim : 0.094742\n",
            "BE_band_width_[Hz] : 200000\n",
            "BE_bins_per_band : 0\n",
            "BE_center_frequency_[Hz] : 1310000\n",
            "BE_desired_duration_[s] : 0.004\n",
            "BE_phase_content : chirp-sinc hybrid\n",
            "BE_phase_variation : 1\n",
            "BE_points_per_BE_wave : 0\n",
            "BE_repeats : 4\n",
            "FORC_V_high1_[V] : 1\n",
            "FORC_V_high2_[V] : 10\n",
            "FORC_V_low1_[V] : -1\n",
            "FORC_V_low2_[V] : -10\n",
            "FORC_num_of_FORC_cycles : 1\n",
            "FORC_num_of_FORC_repeats : 1\n",
            "File_MDAQ_version : MDAQ_VS_090915_01\n",
            "File_date_and_time : 18-Sep-2015 18:32:14\n",
            "File_file_name : SP128_NSO\n",
            "File_file_path : C:\\Users\\Asylum User\\Documents\\Users\\Agar\\SP128_NSO\\\n",
            "File_file_suffix : 99\n",
            "IO_AO_amplifier : 10\n",
            "IO_AO_range_[V] : +/- 10\n",
            "IO_Analog_Input_1 : +/- .1V, FFT\n",
            "IO_Analog_Input_2 : off\n",
            "IO_Analog_Input_3 : off\n",
            "IO_Analog_Input_4 : off\n",
            "IO_DAQ_platform : NI 6115\n",
            "IO_rate_[Hz] : 4000000\n",
            "VS_amplitude_[V] : 16\n",
            "VS_cycle_fraction : full\n",
            "VS_cycle_phase_shift : 0\n",
            "VS_measure_in_field_loops : in and out-of-field\n",
            "VS_mode : DC modulation mode\n",
            "VS_number_of_cycles : 2\n",
            "VS_offset_[V] : 0\n",
            "VS_read_voltage_[V] : 0\n",
            "VS_set_pulse_amplitude[V] : 0\n",
            "VS_set_pulse_duration[s] : 0.002\n",
            "VS_step_edge_smoothing_[s] : 0.001\n",
            "VS_steps_per_full_cycle : 96\n",
            "data_type : BEPSData\n",
            "grid_/single : grid\n",
            "grid_contact_set_point_[V] : 1\n",
            "grid_current_col : 1\n",
            "grid_current_row : 1\n",
            "grid_cycle_time_[s] : 10\n",
            "grid_measuring : 0\n",
            "grid_moving : 0\n",
            "grid_num_cols : 60\n",
            "grid_num_rows : 60\n",
            "grid_settle_time_[s] : 0.15\n",
            "grid_time_remaining_[h;m;s] : 10\n",
            "grid_total_time_[h;m;s] : 10\n",
            "grid_transit_set_point_[V] : 0.1\n",
            "grid_transit_time_[s] : 0.15\n",
            "num_bins : 165\n",
            "num_pix : 3600\n",
            "num_udvs_steps : 384\n"
          ]
        }
      ],
      "source": [
        "print('Datasets and datagroups within the file:\\n------------------------------------')\n",
        "print_tree(h5_f.file)\n",
        " \n",
        "print('\\nThe main dataset:\\n------------------------------------')\n",
        "print(h5_f)\n",
        "print('\\nThe ancillary datasets:\\n------------------------------------')\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Position_Indices'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Position_Values'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Spectroscopic_Indices'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Spectroscopic_Values'])\n",
        "\n",
        "print('\\nMetadata or attributes in a datagroup\\n------------------------------------')\n",
        "for key in h5_f.file['/Measurement_000'].attrs:\n",
        "    print('{} : {}'.format(key, h5_f.file['/Measurement_000'].attrs[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2ef928e6",
      "metadata": {
        "scrolled": false,
        "id": "2ef928e6"
      },
      "outputs": [],
      "source": [
        "# number of samples per SHO fit\n",
        "num_bins = h5_f['Measurement_000'].attrs['num_bins'] \n",
        "\n",
        "# number of pixels in the image\n",
        "num_pix = h5_f['Measurement_000'].attrs['num_pix'] \n",
        "\n",
        "# number of pixels in x and y dimensions\n",
        "num_pix_1d = int(np.sqrt(num_pix)) \n",
        "\n",
        "# number of DC voltage steps \n",
        "voltage_steps = h5_f['Measurement_000'].attrs['num_udvs_steps']\n",
        "\n",
        "# sampling rate\n",
        "sampling_rate = h5_f['Measurement_000'].attrs['IO_rate_[Hz]']\n",
        "\n",
        "# BE bandwidth\n",
        "be_bandwidth = h5_f['Measurement_000'].attrs['BE_band_width_[Hz]']\n",
        "\n",
        "# BE center frequency\n",
        "be_center_frequency = h5_f['Measurement_000'].attrs['BE_center_frequency_[Hz]']\n",
        "\n",
        "# Frequency Vector in Hz\n",
        "frequency_bin = h5_f['Measurement_000']['Channel_000']['Bin_Frequencies'][:]\n",
        "\n",
        "# Resampled frequency vector\n",
        "wvec_freq = resample(frequency_bin, 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "944ebbdf",
      "metadata": {
        "scrolled": false,
        "id": "944ebbdf"
      },
      "outputs": [],
      "source": [
        "# get raw data (real and imaginary combined)\n",
        "raw_data = h5_f['Measurement_000']['Channel_000']['Raw_Data']\n",
        "# resampling it from 165 to 80 frequency steps\n",
        "raw_data_resampled = resample(np.array(raw_data).reshape(-1 , 165), 80, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ffb1ba4",
      "metadata": {
        "id": "2ffb1ba4"
      },
      "source": [
        "## Conversion of Real/Imaginary to Magnitude/Phase Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f85b0fb5",
      "metadata": {
        "scrolled": false,
        "id": "f85b0fb5"
      },
      "outputs": [],
      "source": [
        "# conversion of raw data (both resampled and full)\n",
        "magnitude_graph_initial_full, phase_graph_initial_full = convert_real_imag(raw_data)\n",
        "magnitude_graph_initial, phase_graph_initial = convert_real_imag(raw_data_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2ce769",
      "metadata": {
        "id": "6f2ce769"
      },
      "source": [
        "## Resampling the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d63f0484",
      "metadata": {
        "scrolled": false,
        "id": "d63f0484"
      },
      "outputs": [],
      "source": [
        "# get real and imaginary components from raw data\n",
        "real = np.real(h5_f['Measurement_000']['Channel_000']['Raw_Data'])\n",
        "imag = np.imag(h5_f['Measurement_000']['Channel_000']['Raw_Data'])\n",
        "\n",
        "# resample both real and imaginary components \n",
        "real_resample = resample(real.reshape(num_pix, -1, num_bins), 80, axis=2)\n",
        "imag_resample = resample(imag.reshape(num_pix, -1, num_bins), 80, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a5932a15",
      "metadata": {
        "scrolled": false,
        "id": "a5932a15"
      },
      "outputs": [],
      "source": [
        "# free up the RAM\n",
        "del real\n",
        "del imag\n",
        "gc.collect();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c435249",
      "metadata": {
        "id": "8c435249"
      },
      "source": [
        "### Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "bd752db0",
      "metadata": {
        "scrolled": false,
        "id": "bd752db0"
      },
      "outputs": [],
      "source": [
        "# create a list for parameters\n",
        "fit_results_list = []\n",
        "for sublist in np.array(h5_f['Measurement_000']['Channel_000']['Raw_Data-SHO_Fit_000']['Fit']):\n",
        "    for item in sublist:\n",
        "        for i in item:\n",
        "          fit_results_list.append(i)\n",
        "\n",
        "# flatten parameters list into numpy array\n",
        "fit_results_list = np.array(fit_results_list).reshape(num_pix,voltage_steps,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "85cd0ead",
      "metadata": {
        "scrolled": false,
        "id": "85cd0ead"
      },
      "outputs": [],
      "source": [
        "# scale the fit results with Standard Scaler\n",
        "fit_results_scaler = StandardScaler()\n",
        "scaled_fit_results = fit_results_scaler.fit_transform(fit_results_list.reshape(-1,5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d7122e3",
      "metadata": {
        "id": "1d7122e3"
      },
      "source": [
        "### Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "64f86b30",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64f86b30",
        "outputId": "662885c4-8d8b-473c-8490-4cc5f179aa27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean =  -6.855169e-06 STD =  0.0026878386\n",
            "mean =  0.00013161483 STD =  0.0027575183\n",
            "(1382400, 4)\n"
          ]
        }
      ],
      "source": [
        "# scale the real component of input data\n",
        "scaler_real = global_standard_scaler()\n",
        "scaled_data_real = scaler_real.fit_transform(real_resample).reshape(-1, 80)\n",
        "\n",
        "# scale the imaginary component of input data\n",
        "scaler_imag = global_standard_scaler()\n",
        "scaled_data_imag = scaler_imag.fit_transform(imag_resample).reshape(-1, 80)\n",
        "\n",
        "# stack both components\n",
        "data_ = np.stack((scaled_data_real, scaled_data_imag),axis=2)\n",
        "\n",
        "# scale the parameters (now takes only 4 parameters, excluding the R2)\n",
        "params_scaler = StandardScaler()\n",
        "scaled_params = params_scaler.fit_transform(fit_results_list.reshape(-1,5)[:,0:4])\n",
        "\n",
        "# exclude the R2 parameter\n",
        "params = fit_results_list.reshape(-1,5)[:,0:4]\n",
        "print(params.shape)\n",
        "\n",
        "del real_resample\n",
        "del imag_resample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "818aaa98",
      "metadata": {
        "id": "818aaa98"
      },
      "source": [
        "### Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "67d05855",
      "metadata": {
        "scrolled": false,
        "id": "67d05855"
      },
      "outputs": [],
      "source": [
        "data_train, data_test, params_train, params_test = train_test_split(data_, \n",
        "                                                                    params, \n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4816ca7",
      "metadata": {
        "id": "c4816ca7"
      },
      "source": [
        "## Tensorflow/HLS4ML Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "efbe6ffe",
      "metadata": {
        "scrolled": false,
        "id": "efbe6ffe"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.set_floatx('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e312763a",
      "metadata": {
        "id": "e312763a"
      },
      "source": [
        "### Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "9928a43d",
      "metadata": {
        "id": "9928a43d"
      },
      "outputs": [],
      "source": [
        "def SHO_fitting_function(params):\n",
        "    amp = tf.expand_dims(params[:, 0], 1)\n",
        "    w_0 = tf.expand_dims(params[:, 1], 1)\n",
        "    qf  = tf.expand_dims(params[:, 2], 1)\n",
        "    phi = tf.expand_dims(params[:, 3], 1)\n",
        "    \n",
        "    amp = tf.cast(amp, tf.complex128)\n",
        "    w_0 = tf.cast(w_0, tf.complex128)\n",
        "    qf = tf.cast(qf, tf.complex128)\n",
        "    phi = tf.cast(phi, tf.complex128)\n",
        "    frequency = tf.cast(wvec_freq, tf.complex128)\n",
        "    \n",
        "    numer = amp * tf.math.exp((1.j) * phi) * tf.math.square(w_0)\n",
        "    den_1 = tf.math.square(frequency)\n",
        "    den_2 = (1.j) * frequency * w_0 / qf\n",
        "    den_3 = tf.math.square(w_0)\n",
        "    \n",
        "    den = den_1 - den_2 - den_3\n",
        "    func = numer / den\n",
        "    \n",
        "    real = tf.math.real(func)\n",
        "    real_scaled = tf.divide(tf.subtract(tf.cast(real, tf.float32), \\\n",
        "                            tf.convert_to_tensor(scaler_real.mean, dtype=tf.float32)),\\\n",
        "                             tf.convert_to_tensor(scaler_real.std, dtype=tf.float32))\n",
        "    \n",
        "    imag = tf.math.imag(func)\n",
        "    imag_scaled = tf.divide(tf.subtract(tf.cast(imag, tf.complex128), \\\n",
        "                        tf.convert_to_tensor(scaler_imag.mean, dtype=tf.complex128)),\\\n",
        "                          tf.convert_to_tensor(scaler_imag.std, dtype=tf.complex128))\n",
        "    \n",
        "    func = tf.stack((real_scaled, tf.cast(imag_scaled, tf.float32)), axis=2)\n",
        "    return func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8cb4824b",
      "metadata": {
        "id": "8cb4824b"
      },
      "outputs": [],
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    y_true = SHO_fitting_function(y_true)\n",
        "    unscaled_params = y_pred * tf.convert_to_tensor(np.sqrt(params_scaler.var_[0:4]), dtype=tf.float32) + tf.convert_to_tensor(params_scaler.mean_[0:4], dtype=tf.float32)\n",
        "    y_pred = SHO_fitting_function(unscaled_params)\n",
        "    mse = tf.math.square(y_true-y_pred)\n",
        "    return tf.reduce_mean(mse, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c09851",
      "metadata": {
        "id": "d0c09851"
      },
      "source": [
        "### Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "fc48dd33",
      "metadata": {
        "scrolled": false,
        "id": "fc48dd33"
      },
      "outputs": [],
      "source": [
        "def Conv1D_Block(x, time_step, kernel_size):\n",
        "    x = Conv1D(time_step, kernel_size, padding='same')(x)\n",
        "    x = Activation(selu)(x)\n",
        "    return x\n",
        "\n",
        "def SHO_Model(n_step=80):\n",
        "    original_input = tf.keras.Input(shape=(n_step, 2))\n",
        "    x = Conv1D_Block(original_input, 8, 7)\n",
        "    x = Conv1D_Block(x, 6, 7)\n",
        "    x = Conv1D_Block(x, 4, 5)\n",
        "    \n",
        "    xfc = Flatten()(x)\n",
        "    xfc = Dense(20, selu)(xfc)\n",
        "    xfc = Dense(20, selu)(xfc)\n",
        "    \n",
        "    x = MaxPool1D(2, padding='same')(x)\n",
        "    x = Conv1D_Block(x, 2, 5)\n",
        "    x = Conv1D_Block(x, 4, 5)\n",
        "    x = Conv1D_Block(x, 4, 5)\n",
        "    x = Conv1D_Block(x, 4, 5)\n",
        "    x = Conv1D_Block(x, 4, 5)\n",
        "    x = Conv1D_Block(x, 4, 5)\n",
        "    x = AvgPool1D(2, padding='same')(x)\n",
        "    x = Conv1D_Block(x, 4, 3)\n",
        "    x = AvgPool1D(2, padding='same')(x)\n",
        "    x = Conv1D_Block(x, 4, 3)\n",
        "    x = AvgPool1D(2, padding='same')(x)\n",
        "    \n",
        "    cnn_flat = Flatten()(x)\n",
        "    encoded = tf.concat([cnn_flat, xfc], 1)\n",
        "    encoded = tf.keras.layers.concatenate([cnn_flat, xfc], 1)\n",
        "    embedding = Dense(16, selu)(encoded)\n",
        "    embedding = Dense(8, selu)(embedding)\n",
        "    embedding = Dense(4, selu)(embedding)\n",
        "    \n",
        "    model = Model(original_input, embedding, name='SHO_Model')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b55e0741",
      "metadata": {
        "scrolled": false,
        "id": "b55e0741"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((data_train, params_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((data_test, params_test))\n",
        "BATCH_SIZE = 1024\n",
        "SHUFFLE_BUFFER_SIZE = 64\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "44c84e13",
      "metadata": {
        "scrolled": false,
        "id": "44c84e13"
      },
      "outputs": [],
      "source": [
        "model = SHO_Model()\n",
        "adam = Adam(3e-5)\n",
        "sgd = SGD()\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "model.compile(optimizer=sgd, loss=custom_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "29ca7bff",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ca7bff",
        "outputId": "a4101392-87e5-47f2-94d1-920817ee432a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"SHO_Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 80, 2)]      0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 80, 8)        120         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 80, 8)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 80, 6)        342         ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 80, 6)        0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 80, 4)        124         ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 80, 4)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 40, 4)        0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 40, 2)        42          ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 40, 2)        0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 40, 4)        44          ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 40, 4)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 40, 4)        84          ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 40, 4)        0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 40, 4)        84          ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 40, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 40, 4)        84          ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 40, 4)        0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 40, 4)        84          ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 40, 4)        0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 20, 4)       0           ['activation_8[0][0]']           \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 20, 4)        52          ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 20, 4)        0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " average_pooling1d_1 (AveragePo  (None, 10, 4)       0           ['activation_9[0][0]']           \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 10, 4)        52          ['average_pooling1d_1[0][0]']    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 10, 4)        0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 320)          0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " average_pooling1d_2 (AveragePo  (None, 5, 4)        0           ['activation_10[0][0]']          \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 20)           6420        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 20)           0           ['average_pooling1d_2[0][0]']    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 20)           420         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 40)           0           ['flatten_1[0][0]',              \n",
            "                                                                  'dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 16)           656         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 8)            136         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 4)            36          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,780\n",
            "Trainable params: 8,780\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "df958ac1",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df958ac1",
        "outputId": "5841ae13-df2f-414f-c81b-77ec7754b12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1d: 112\n",
            "conv1d_1: 336\n",
            "conv1d_2: 120\n",
            "conv1d_3: 40\n",
            "conv1d_4: 40\n",
            "conv1d_5: 80\n",
            "conv1d_6: 80\n",
            "conv1d_7: 80\n",
            "conv1d_8: 80\n",
            "conv1d_9: 48\n",
            "conv1d_10: 48\n",
            "dense: 6400\n",
            "Layer dense is too large (6400), are you sure you want to train?\n",
            "dense_1: 400\n",
            "dense_2: 640\n",
            "dense_3: 128\n",
            "dense_4: 32\n"
          ]
        }
      ],
      "source": [
        "for layer in model.layers:\n",
        "    if layer.__class__.__name__ in ['Conv1D', 'Dense']:\n",
        "        w = layer.get_weights()[0]\n",
        "        layersize = np.prod(w.shape)\n",
        "        print(\"{}: {}\".format(layer.name,layersize)) # 0 = weights, 1 = biases\n",
        "        if (layersize > 4096): # assuming that shape[0] is batch, i.e., 'None'\n",
        "            print(\"Layer {} is too large ({}), are you sure you want to train?\".format(layer.name,layersize))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "24696f23",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24696f23",
        "outputId": "b0689b8e-c2cc-45d3-f3be-8d19190de5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1080/1080 [==============================] - 23s 10ms/step - loss: 0.3084\n",
            "Epoch 2/5\n",
            "1080/1080 [==============================] - 10s 10ms/step - loss: 0.0352\n",
            "Epoch 3/5\n",
            "1080/1080 [==============================] - 10s 10ms/step - loss: 0.0189\n",
            "Epoch 4/5\n",
            "1080/1080 [==============================] - 11s 10ms/step - loss: 0.0144\n",
            "Epoch 5/5\n",
            "1080/1080 [==============================] - 10s 10ms/step - loss: 0.0119\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa2c0db2950>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "model.fit(train_dataset, epochs=5, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "93a55a2e",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93a55a2e",
        "outputId": "3cda23f4-5475-4d95-b71e-adbff7cd8b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/tf_sho_model_v2/assets\n"
          ]
        }
      ],
      "source": [
        "model.save('saved_models/tf_sho_model_v2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDWbzOj7KfQ9",
        "outputId": "65c2d528-fc1a-43e3-fddc-f56267f8540c"
      },
      "id": "IDWbzOj7KfQ9",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30606"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6268969",
      "metadata": {
        "id": "a6268969"
      },
      "source": [
        "### Pruned and Quantized Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44393490",
      "metadata": {
        "id": "44393490"
      },
      "source": [
        "#### AutoQKeras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/google/qkeras.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM9242TzFyey",
        "outputId": "b6aa44b0-39e2-4ce9-b117-123b6a4954c6"
      },
      "id": "tM9242TzFyey",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qkeras'...\n",
            "remote: Enumerating objects: 2137, done.\u001b[K\n",
            "remote: Counting objects: 100% (669/669), done.\u001b[K\n",
            "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "remote: Total 2137 (delta 455), reused 591 (delta 408), pack-reused 1468\u001b[K\n",
            "Receiving objects: 100% (2137/2137), 902.59 KiB | 2.72 MiB/s, done.\n",
            "Resolving deltas: 100% (1493/1493), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOGd9NqOGHwp",
        "outputId": "4ae9dd8f-0091-4875-f885-f1f1bead5e5f"
      },
      "id": "mOGd9NqOGHwp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34mAssets\u001b[0m/        \u001b[01;34mBGlib\u001b[0m/         \u001b[01;34mnotebooks\u001b[0m/            \u001b[01;34msaved_models\u001b[0m/\n",
            " \u001b[01;34mautoq_cnn\u001b[0m/     \u001b[01;34mcodes\u001b[0m/         pruned_cnn_model.h5  \u001b[01;34m'Trained Models'\u001b[0m/\n",
            " \u001b[01;34mautoq_cnn_1\u001b[0m/   data_file.h5   \u001b[01;34mqkeras\u001b[0m/\n",
            " \u001b[01;34mautoq_cnn_2\u001b[0m/   \u001b[01;34mLaTeX\u001b[0m/         README.md\n",
            " \u001b[01;34mautoq_cnn_3\u001b[0m/   LICENSE        requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from qkeras.qkeras.autoqkeras import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "SnTT8fWNGJu2",
        "outputId": "50605dce-b5b1-4731-a391-5358cff714ee"
      },
      "id": "SnTT8fWNGJu2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-936952c0584a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mqkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoqkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qkeras.qkeras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "f0T6aj_YFzO2"
      },
      "id": "f0T6aj_YFzO2"
    },
    {
      "cell_type": "code",
      "source": [
        "# model = tf.keras.models.load_model('saved_models/tf_sho_model_v2', compile=False)"
      ],
      "metadata": {
        "id": "pS7urAVAK4WB"
      },
      "id": "pS7urAVAK4WB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "875f6cb5",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "875f6cb5",
        "outputId": "1c0a65e9-dffa-406e-9b0a-37592d1ffebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/qkeras/qtools/qgraph.py:189: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "operation count for <keras.layers.pooling.AveragePooling1D object at 0x7fa2c336b610> is defaulted to 0\n",
            "operation count for <keras.layers.pooling.AveragePooling1D object at 0x7fa2d0dbb0d0> is defaulted to 0\n",
            "operation count for <keras.layers.pooling.AveragePooling1D object at 0x7fa2c33662d0> is defaulted to 0\n",
            "{\n",
            "    \"source_quantizers\": [\n",
            "        {\n",
            "            \"quantizer_type\": \"quantized_bits\",\n",
            "            \"bits\": 16,\n",
            "            \"int_bits\": 6,\n",
            "            \"is_signed\": true\n",
            "        }\n",
            "    ],\n",
            "    \"conv1d\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"quantized_bits\",\n",
            "                \"bits\": 16,\n",
            "                \"int_bits\": 6,\n",
            "                \"is_signed\": true\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                7,\n",
            "                2,\n",
            "                8\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 8\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                80,\n",
            "                8\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 8960\n",
            "    },\n",
            "    \"activation\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                80,\n",
            "                8\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 640\n",
            "    },\n",
            "    \"conv1d_1\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                7,\n",
            "                8,\n",
            "                6\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 6\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                80,\n",
            "                6\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 26880\n",
            "    },\n",
            "    \"activation_1\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                80,\n",
            "                6\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 480\n",
            "    },\n",
            "    \"conv1d_2\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                5,\n",
            "                6,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                80,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 9600\n",
            "    },\n",
            "    \"activation_2\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                80,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 320\n",
            "    },\n",
            "    \"max_pooling1d\": {\n",
            "        \"layer_type\": \"MaxPooling1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 320\n",
            "    },\n",
            "    \"flatten\": {\n",
            "        \"layer_type\": \"Flatten\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                320\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 320\n",
            "    },\n",
            "    \"conv1d_3\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                5,\n",
            "                4,\n",
            "                2\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 2\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                2\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 1600\n",
            "    },\n",
            "    \"dense\": {\n",
            "        \"layer_type\": \"Dense\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                320,\n",
            "                20\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 20\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                20\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 6400\n",
            "    },\n",
            "    \"activation_3\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                2\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 80\n",
            "    },\n",
            "    \"dense_1\": {\n",
            "        \"layer_type\": \"Dense\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                20,\n",
            "                20\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 20\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                20\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 400\n",
            "    },\n",
            "    \"conv1d_4\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                5,\n",
            "                2,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 1600\n",
            "    },\n",
            "    \"activation_4\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 160\n",
            "    },\n",
            "    \"conv1d_5\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                5,\n",
            "                4,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 3200\n",
            "    },\n",
            "    \"activation_5\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 160\n",
            "    },\n",
            "    \"conv1d_6\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                5,\n",
            "                4,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 3200\n",
            "    },\n",
            "    \"activation_6\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 160\n",
            "    },\n",
            "    \"conv1d_7\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                5,\n",
            "                4,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 3200\n",
            "    },\n",
            "    \"activation_7\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 160\n",
            "    },\n",
            "    \"conv1d_8\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                5,\n",
            "                4,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 3200\n",
            "    },\n",
            "    \"activation_8\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 160\n",
            "    },\n",
            "    \"average_pooling1d\": {\n",
            "        \"layer_type\": \"AveragePooling1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                20,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 0\n",
            "    },\n",
            "    \"conv1d_9\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                3,\n",
            "                4,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                20,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 960\n",
            "    },\n",
            "    \"activation_9\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                20,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 80\n",
            "    },\n",
            "    \"average_pooling1d_1\": {\n",
            "        \"layer_type\": \"AveragePooling1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                10,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 0\n",
            "    },\n",
            "    \"conv1d_10\": {\n",
            "        \"layer_type\": \"Conv1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                3,\n",
            "                4,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                10,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 480\n",
            "    },\n",
            "    \"activation_10\": {\n",
            "        \"layer_type\": \"Activation\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                10,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 40\n",
            "    },\n",
            "    \"average_pooling1d_2\": {\n",
            "        \"layer_type\": \"AveragePooling1D\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                5,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 0\n",
            "    },\n",
            "    \"flatten_1\": {\n",
            "        \"layer_type\": \"Flatten\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                20\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 20\n",
            "    },\n",
            "    \"concatenate\": {\n",
            "        \"layer_type\": \"Concatenate\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            },\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"Concatenate_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                40\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 20\n",
            "    },\n",
            "    \"dense_2\": {\n",
            "        \"layer_type\": \"Dense\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                40,\n",
            "                16\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 16\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                16\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 640\n",
            "    },\n",
            "    \"dense_3\": {\n",
            "        \"layer_type\": \"Dense\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                16,\n",
            "                8\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 8\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                8\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 128\n",
            "    },\n",
            "    \"dense_4\": {\n",
            "        \"layer_type\": \"Dense\",\n",
            "        \"input_quantizer_list\": [\n",
            "            {\n",
            "                \"quantizer_type\": \"floating_point\",\n",
            "                \"bits\": 16\n",
            "            }\n",
            "        ],\n",
            "        \"weight_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                8,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"bias_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": 4\n",
            "        },\n",
            "        \"multiplier\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"mul\"\n",
            "        },\n",
            "        \"accumulator\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"op_type\": \"add\"\n",
            "        },\n",
            "        \"output_quantizer\": {\n",
            "            \"quantizer_type\": \"floating_point\",\n",
            "            \"bits\": 16,\n",
            "            \"shape\": [\n",
            "                -1,\n",
            "                4\n",
            "            ]\n",
            "        },\n",
            "        \"operation_count\": 32\n",
            "    }\n",
            "}\n",
            "{'activation': {'energy': {'inputs': 0.0,\n",
            "                           'op_cost': 0.0,\n",
            "                           'outputs': 0.0,\n",
            "                           'parameters': 0.0},\n",
            "                'total': 0.0},\n",
            " 'activation_1': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'activation_10': {'energy': {'inputs': 0.0,\n",
            "                              'op_cost': 0.0,\n",
            "                              'outputs': 0.0,\n",
            "                              'parameters': 0.0},\n",
            "                   'total': 0.0},\n",
            " 'activation_2': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'activation_3': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'activation_4': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'activation_5': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'activation_6': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'activation_7': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'activation_8': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'activation_9': {'energy': {'inputs': 0.0,\n",
            "                             'op_cost': 0.0,\n",
            "                             'outputs': 0.0,\n",
            "                             'parameters': 0.0},\n",
            "                  'total': 0.0},\n",
            " 'average_pooling1d': {'energy': {'inputs': 0.0,\n",
            "                                  'op_cost': 0.0,\n",
            "                                  'outputs': 0.0,\n",
            "                                  'parameters': 0.0},\n",
            "                       'total': 0.0},\n",
            " 'average_pooling1d_1': {'energy': {'inputs': 0.0,\n",
            "                                    'op_cost': 0.0,\n",
            "                                    'outputs': 0.0,\n",
            "                                    'parameters': 0.0},\n",
            "                         'total': 0.0},\n",
            " 'average_pooling1d_2': {'energy': {'inputs': 0.0,\n",
            "                                    'op_cost': 0.0,\n",
            "                                    'outputs': 0.0,\n",
            "                                    'parameters': 0.0},\n",
            "                         'total': 0.0},\n",
            " 'concatenate': {'energy': {'inputs': 0.0,\n",
            "                            'op_cost': 0.0,\n",
            "                            'outputs': 0.0,\n",
            "                            'parameters': 0.0},\n",
            "                 'total': 0.0},\n",
            " 'conv1d': {'energy': {'inputs': 152.15,\n",
            "                       'op_cost': 13440.0,\n",
            "                       'outputs': 0.0,\n",
            "                       'parameters': 0.0},\n",
            "            'total': 13592.15},\n",
            " 'conv1d_1': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 40320.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 40320.0},\n",
            " 'conv1d_10': {'energy': {'inputs': 0.0,\n",
            "                          'op_cost': 720.0,\n",
            "                          'outputs': 0.0,\n",
            "                          'parameters': 0.0},\n",
            "               'total': 720.0},\n",
            " 'conv1d_2': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 14400.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 14400.0},\n",
            " 'conv1d_3': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 2400.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 2400.0},\n",
            " 'conv1d_4': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 2400.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 2400.0},\n",
            " 'conv1d_5': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 4800.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 4800.0},\n",
            " 'conv1d_6': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 4800.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 4800.0},\n",
            " 'conv1d_7': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 4800.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 4800.0},\n",
            " 'conv1d_8': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 4800.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 4800.0},\n",
            " 'conv1d_9': {'energy': {'inputs': 0.0,\n",
            "                         'op_cost': 1440.0,\n",
            "                         'outputs': 0.0,\n",
            "                         'parameters': 0.0},\n",
            "              'total': 1440.0},\n",
            " 'dense': {'energy': {'inputs': 0.0,\n",
            "                      'op_cost': 9600.0,\n",
            "                      'outputs': 0.0,\n",
            "                      'parameters': 0.0},\n",
            "           'total': 9600.0},\n",
            " 'dense_1': {'energy': {'inputs': 0.0,\n",
            "                        'op_cost': 600.0,\n",
            "                        'outputs': 0.0,\n",
            "                        'parameters': 0.0},\n",
            "             'total': 600.0},\n",
            " 'dense_2': {'energy': {'inputs': 0.0,\n",
            "                        'op_cost': 960.0,\n",
            "                        'outputs': 0.0,\n",
            "                        'parameters': 0.0},\n",
            "             'total': 960.0},\n",
            " 'dense_3': {'energy': {'inputs': 0.0,\n",
            "                        'op_cost': 192.0,\n",
            "                        'outputs': 0.0,\n",
            "                        'parameters': 0.0},\n",
            "             'total': 192.0},\n",
            " 'dense_4': {'energy': {'inputs': 0.0,\n",
            "                        'op_cost': 48.0,\n",
            "                        'outputs': 3.8,\n",
            "                        'parameters': 0.0},\n",
            "             'total': 48.0},\n",
            " 'flatten': {'energy': {'inputs': 0.0,\n",
            "                        'op_cost': 0.0,\n",
            "                        'outputs': 0.0,\n",
            "                        'parameters': 0.0},\n",
            "             'total': 0.0},\n",
            " 'flatten_1': {'energy': {'inputs': 0.0,\n",
            "                          'op_cost': 0.0,\n",
            "                          'outputs': 0.0,\n",
            "                          'parameters': 0.0},\n",
            "               'total': 0.0},\n",
            " 'max_pooling1d': {'energy': {'inputs': 0.0,\n",
            "                              'op_cost': 0.0,\n",
            "                              'outputs': 0.0,\n",
            "                              'parameters': 0.0},\n",
            "                   'total': 0.0}}\n",
            "\n",
            "Total energy: 0.105872 uJ\n"
          ]
        }
      ],
      "source": [
        "from qkeras import print_qstats\n",
        "# for automatic quantization\n",
        "import pprint\n",
        "from qkeras.autoqkeras import *\n",
        "from qkeras import *\n",
        "from qkeras.utils import model_quantize\n",
        "\n",
        "from qkeras.qtools import run_qtools\n",
        "from qkeras.qtools import settings as qtools_settings\n",
        "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
        "from qkeras import quantized_bits\n",
        "from qkeras import QDense, QActivation\n",
        "\n",
        "q = run_qtools.QTools(model, \n",
        "                      process=\"horowitz\", \n",
        "                      source_quantizers=[quantized_bits(16, 5, 1)], \n",
        "                      is_inference=True, \n",
        "                      weights_path=None,\n",
        "                      keras_quantizer=\"fp16\",\n",
        "                      keras_accumulator=\"fp16\", \n",
        "                      for_reference=False)\n",
        "q.qtools_stats_print()\n",
        "\n",
        "energy_dict = q.pe(\n",
        "    weights_on_memory=\"fixed\",\n",
        "    activations_on_memory=\"fixed\",\n",
        "    min_sram_size=8*16*1024*1024,\n",
        "    rd_wr_on_io=False)\n",
        "\n",
        "# get stats of energy distribution in each layer\n",
        "energy_profile = q.extract_energy_profile(\n",
        "    qtools_settings.cfg.include_energy, energy_dict)\n",
        "# extract sum of energy of each layer according to the rule specified in\n",
        "# qtools_settings.cfg.include_energy\n",
        "total_energy = q.extract_energy_sum(\n",
        "    qtools_settings.cfg.include_energy, energy_dict)\n",
        "\n",
        "pprint.pprint(energy_profile)\n",
        "print()\n",
        "\n",
        "print(\"Total energy: {:.6f} uJ\".format(total_energy / 1000000.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "354d2cf1",
      "metadata": {
        "id": "354d2cf1"
      },
      "outputs": [],
      "source": [
        "# These are the quantizers we'll test in the bayesian optimization\n",
        "quantization_config = {\n",
        "        \"kernel\": {\n",
        "                \"quantized_bits(2,0,1,alpha=1.0)\": 2,\n",
        "                \"quantized_bits(4,0,1,alpha=1.0)\": 4,\n",
        "                \"quantized_bits(6,0,1,alpha=1.0)\": 6,\n",
        "                \"quantized_bits(8,0,1,alpha=1.0)\": 8,\n",
        "        },\n",
        "        \"bias\": {\n",
        "                \"quantized_bits(2,0,1,alpha=1.0)\": 2,\n",
        "                \"quantized_bits(4,0,1,alpha=1.0)\": 4,\n",
        "                \"quantized_bits(6,0,1,alpha=1.0)\": 6,\n",
        "                \"quantized_bits(8,0,1,alpha=1.0)\": 8,\n",
        "        },\n",
        "        \"activation\": {\n",
        "                \"quantized_relu(3,1)\": 3,\n",
        "                \"quantized_relu(4,2)\": 4,\n",
        "                \"quantized_relu(8,2)\": 8,\n",
        "                \"quantized_relu(8,4)\": 8,\n",
        "                \"quantized_relu(16,6)\": 16\n",
        "        },\n",
        "        \"linear\": {\n",
        "                \"quantized_bits(2,0,1,alpha=1.0)\": 2,\n",
        "                \"quantized_bits(4,0,1,alpha=1.0)\": 4,\n",
        "                \"quantized_bits(6,0,1,alpha=1.0)\": 6,\n",
        "                \"quantized_bits(8,0,1,alpha=1.0)\": 8,\n",
        "        }\n",
        "}\n",
        "\n",
        "# These are the layer types we will quantize\n",
        "limit = {\n",
        "    \"Dense\": [8, 8, 16],\n",
        "    \"Conv1D\": [8, 8, 16],\n",
        "    \"Activation\": [16],\n",
        "}\n",
        "\n",
        "# Use this if you want to minimize the model bit size\n",
        "goal_bits = {\n",
        "    \"type\": \"bits\",\n",
        "          \"params\": {\n",
        "              \"delta_p\": 2.0, # We tolerate up to a +2% accuracy change\n",
        "              \"delta_n\": 2.0, # We tolerate down to a -2% accuracy change\n",
        "              \"rate\": 2.0,    # We want a x2 times smaller model\n",
        "              \"stress\": 0.5,  # Force the reference model size to be smaller by setting stress<1\n",
        "              \"input_bits\": 8,\n",
        "              \"output_bits\": 8,\n",
        "              \"ref_bits\": 8,\n",
        "              \"config\": {\n",
        "                  \"default\": [\"parameters\", \"activations\"]\n",
        "              }\n",
        "          }\n",
        "}\n",
        "\n",
        "# Use this if you want to minimize the model energy consumption\n",
        "goal_energy = {\n",
        "    \"type\": \"energy\",\n",
        "    \"params\": {\n",
        "        \"delta_p\": 2.0,\n",
        "        \"delta_n\": 2.0,\n",
        "        \"rate\": 2.0,\n",
        "        \"stress\": 0.5,\n",
        "        \"process\": \"horowitz\",\n",
        "        \"parameters_on_memory\": [\"sram\", \"sram\"],\n",
        "        \"activations_on_memory\": [\"sram\", \"sram\"],\n",
        "        \"rd_wr_on_io\": [False, False],\n",
        "        \"min_sram_size\": [0, 0],\n",
        "        \"source_quantizers\": [\"fp32\"],\n",
        "        \"reference_internal\": \"int8\",\n",
        "        \"reference_accumulator\": \"int32\"\n",
        "        }\n",
        "}\n",
        "\n",
        "run_config = {\n",
        "        \"goal\": goal_bits,\n",
        "        \"quantization_config\": quantization_config,\n",
        "        \"learning_rate_optimizer\": False,\n",
        "        \"transfer_weights\": False, # Randomely initialize weights\n",
        "        \"mode\": \"bayesian\", # This can be bayesian,random,hyperband\n",
        "        \"seed\": 42,\n",
        "        \"limit\": limit,\n",
        "        \"tune_filters\": \"layer\",\n",
        "        \"tune_filters_exceptions\": \"^output\",\n",
        "        \"distribution_strategy\": None,\n",
        "        \"layer_indexes\": range(1, len(model.layers)-1),\n",
        "        \"max_trials\": 10 # Let's just do 5 trials for this demonstrator, ideally you should do as many as possible\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "82b3fd15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82b3fd15",
        "outputId": "c8414649-20dd-4f79-c4cd-22615127ee64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 02m 36s]\n",
            "val_score: 0.036302149295806885\n",
            "\n",
            "Best val_score So Far: 0.715680718421936\n",
            "Total elapsed time: 00h 32m 15s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate: 0.009999999776482582\n",
            "Model: \"SHO_Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 80, 2)]      0           []                               \n",
            "                                                                                                  \n",
            " conv1d (QConv1D)               (None, 80, 16)       240         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " activation (QActivation)       (None, 80, 16)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (QConv1D)             (None, 80, 12)       1356        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " activation_1 (QActivation)     (None, 80, 12)       0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_2 (QConv1D)             (None, 80, 6)        366         ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " activation_2 (QActivation)     (None, 80, 6)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 40, 6)        0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (QConv1D)             (None, 40, 1)        31          ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " activation_3 (QActivation)     (None, 40, 1)        0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_4 (QConv1D)             (None, 40, 2)        12          ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " activation_4 (QActivation)     (None, 40, 2)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (QConv1D)             (None, 40, 6)        66          ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " activation_5 (QActivation)     (None, 40, 6)        0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_6 (QConv1D)             (None, 40, 4)        124         ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " activation_6 (QActivation)     (None, 40, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (QConv1D)             (None, 40, 2)        42          ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " activation_7 (QActivation)     (None, 40, 2)        0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_8 (QConv1D)             (None, 40, 3)        33          ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " activation_8 (QActivation)     (None, 40, 3)        0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 20, 3)       0           ['activation_8[0][0]']           \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " conv1d_9 (QConv1D)             (None, 20, 3)        30          ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " activation_9 (QActivation)     (None, 20, 3)        0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " average_pooling1d_1 (AveragePo  (None, 10, 3)       0           ['activation_9[0][0]']           \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " conv1d_10 (QConv1D)            (None, 10, 8)        80          ['average_pooling1d_1[0][0]']    \n",
            "                                                                                                  \n",
            " activation_10 (QActivation)    (None, 10, 8)        0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 480)          0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " average_pooling1d_2 (AveragePo  (None, 5, 8)        0           ['activation_10[0][0]']          \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " dense (QDense)                 (None, 10)           4810        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 40)           0           ['average_pooling1d_2[0][0]']    \n",
            "                                                                                                  \n",
            " dense_1 (QDense)               (None, 40)           440         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 80)           0           ['flatten_1[0][0]',              \n",
            "                                                                  'dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (QDense)               (None, 12)           972         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (QDense)               (None, 4)            52          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 4)            20          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,674\n",
            "Trainable params: 8,674\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "stats: delta_p=0.02 delta_n=0.02 rate=2.0 trial_size=61142 reference_size=45792\n",
            "       delta=-0.83%\n",
            "       a_bits=28680/21344 (34.37%) p_bits=32462/70240 (-53.78%)\n",
            "       total=61142/91584 (-33.24%)\n",
            "conv1d               f=16 quantized_bits(2,0,1,alpha=1.0) quantized_bits(4,0,1,alpha=1.0) \n",
            "activation           quantized_relu(8,2)\n",
            "conv1d_1             f=12 quantized_bits(2,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) \n",
            "activation_1         quantized_relu(8,2)\n",
            "conv1d_2             f=6 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) \n",
            "activation_2         quantized_relu(3,1)\n",
            "conv1d_3             f=1 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) \n",
            "activation_3         quantized_relu(3,1)\n",
            "conv1d_4             f=2 quantized_bits(2,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) \n",
            "activation_4         quantized_relu(3,1)\n",
            "conv1d_5             f=6 quantized_bits(4,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) \n",
            "activation_5         quantized_relu(16,6)\n",
            "conv1d_6             f=4 quantized_bits(6,0,1,alpha=1.0) quantized_bits(4,0,1,alpha=1.0) \n",
            "activation_6         quantized_relu(3,1)\n",
            "conv1d_7             f=2 quantized_bits(2,0,1,alpha=1.0) quantized_bits(6,0,1,alpha=1.0) \n",
            "activation_7         quantized_relu(16,6)\n",
            "conv1d_8             f=3 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) \n",
            "activation_8         quantized_relu(8,4)\n",
            "conv1d_9             f=3 quantized_bits(4,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) \n",
            "activation_9         quantized_relu(4,2)\n",
            "conv1d_10            f=8 quantized_bits(6,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) \n",
            "activation_10        quantized_relu(4,2)\n",
            "dense                u=10 quantized_bits(4,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) act=<function selu at 0x7fa359c3ecb0>\n",
            "dense_1              u=40 quantized_bits(6,0,1,alpha=1.0) quantized_bits(4,0,1,alpha=1.0) act=<function selu at 0x7fa359c3ecb0>\n",
            "dense_2              u=12 quantized_bits(2,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) act=<function selu at 0x7fa359c3ecb0>\n",
            "dense_3              u=4 quantized_bits(2,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) act=<function selu at 0x7fa359c3ecb0>\n",
            "\n",
            "conv1d               f=16 quantized_bits(2,0,1,alpha=1.0) quantized_bits(4,0,1,alpha=1.0) \n",
            "activation           quantized_relu(8,2)\n",
            "conv1d_1             f=12 quantized_bits(2,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) \n",
            "activation_1         quantized_relu(8,2)\n",
            "conv1d_2             f=6 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) \n",
            "activation_2         quantized_relu(3,1)\n",
            "conv1d_3             f=1 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) \n",
            "activation_3         quantized_relu(3,1)\n",
            "conv1d_4             f=2 quantized_bits(2,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) \n",
            "activation_4         quantized_relu(3,1)\n",
            "conv1d_5             f=6 quantized_bits(4,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) \n",
            "activation_5         quantized_relu(16,6)\n",
            "conv1d_6             f=4 quantized_bits(6,0,1,alpha=1.0) quantized_bits(4,0,1,alpha=1.0) \n",
            "activation_6         quantized_relu(3,1)\n",
            "conv1d_7             f=2 quantized_bits(2,0,1,alpha=1.0) quantized_bits(6,0,1,alpha=1.0) \n",
            "activation_7         quantized_relu(16,6)\n",
            "conv1d_8             f=3 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) \n",
            "activation_8         quantized_relu(8,4)\n",
            "conv1d_9             f=3 quantized_bits(4,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) \n",
            "activation_9         quantized_relu(4,2)\n",
            "conv1d_10            f=8 quantized_bits(6,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) \n",
            "activation_10        quantized_relu(4,2)\n",
            "dense                u=10 quantized_bits(4,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) act=<function selu at 0x7fa359c3ecb0>\n",
            "dense_1              u=40 quantized_bits(6,0,1,alpha=1.0) quantized_bits(4,0,1,alpha=1.0) act=<function selu at 0x7fa359c3ecb0>\n",
            "dense_2              u=12 quantized_bits(2,0,1,alpha=1.0) quantized_bits(8,0,1,alpha=1.0) act=<function selu at 0x7fa359c3ecb0>\n",
            "dense_3              u=4 quantized_bits(2,0,1,alpha=1.0) quantized_bits(2,0,1,alpha=1.0) act=<function selu at 0x7fa359c3ecb0>\n",
            "\n",
            "Epoch 1/15\n",
            "1080/1080 [==============================] - 24s 18ms/step - loss: 0.4456 - custom_loss: 0.4456 - trial: 61142.9844 - score: 0.4418 - val_loss: 0.2931 - val_custom_loss: 0.2931 - val_trial: 61142.0000 - val_score: 0.2907 - lr: 0.0100\n",
            "Epoch 2/15\n",
            "1080/1080 [==============================] - 18s 16ms/step - loss: 0.1702 - custom_loss: 0.1702 - trial: 61142.9844 - score: 0.1688 - val_loss: 0.0908 - val_custom_loss: 0.0908 - val_trial: 61142.0000 - val_score: 0.0901 - lr: 0.0100\n",
            "Epoch 3/15\n",
            "1080/1080 [==============================] - 18s 16ms/step - loss: 0.1308 - custom_loss: 0.1308 - trial: 61142.9844 - score: 0.1297 - val_loss: 0.0919 - val_custom_loss: 0.0919 - val_trial: 61142.0000 - val_score: 0.0911 - lr: 0.0100\n",
            "Epoch 4/15\n",
            "1080/1080 [==============================] - 15s 13ms/step - loss: 0.1055 - custom_loss: 0.1055 - trial: 61142.9844 - score: 0.1047 - val_loss: 0.0578 - val_custom_loss: 0.0578 - val_trial: 61142.0000 - val_score: 0.0573 - lr: 0.0100\n",
            "Epoch 5/15\n",
            "1080/1080 [==============================] - 14s 13ms/step - loss: 0.0895 - custom_loss: 0.0895 - trial: 61142.9844 - score: 0.0887 - val_loss: 0.2065 - val_custom_loss: 0.2065 - val_trial: 61142.0000 - val_score: 0.2047 - lr: 0.0100\n",
            "Epoch 6/15\n",
            "1080/1080 [==============================] - 15s 13ms/step - loss: 0.0810 - custom_loss: 0.0810 - trial: 61142.9844 - score: 0.0803 - val_loss: 0.0454 - val_custom_loss: 0.0454 - val_trial: 61142.0000 - val_score: 0.0450 - lr: 0.0100\n",
            "Epoch 7/15\n",
            "1080/1080 [==============================] - 15s 14ms/step - loss: 0.0751 - custom_loss: 0.0751 - trial: 61142.9844 - score: 0.0745 - val_loss: 0.0458 - val_custom_loss: 0.0458 - val_trial: 61142.0000 - val_score: 0.0454 - lr: 0.0100\n",
            "Epoch 8/15\n",
            "1080/1080 [==============================] - 15s 13ms/step - loss: 0.0812 - custom_loss: 0.0812 - trial: 61142.9844 - score: 0.0805 - val_loss: 0.0714 - val_custom_loss: 0.0714 - val_trial: 61142.0000 - val_score: 0.0708 - lr: 0.0100\n",
            "Epoch 9/15\n",
            "1080/1080 [==============================] - 15s 13ms/step - loss: 0.0829 - custom_loss: 0.0829 - trial: 61142.9844 - score: 0.0822 - val_loss: 0.0413 - val_custom_loss: 0.0413 - val_trial: 61142.0000 - val_score: 0.0409 - lr: 0.0100\n",
            "Epoch 10/15\n",
            "1080/1080 [==============================] - 15s 13ms/step - loss: 0.0637 - custom_loss: 0.0637 - trial: 61142.9844 - score: 0.0632 - val_loss: 0.0465 - val_custom_loss: 0.0465 - val_trial: 61142.0000 - val_score: 0.0462 - lr: 0.0100\n",
            "Epoch 11/15\n",
            "1080/1080 [==============================] - 14s 13ms/step - loss: 0.0661 - custom_loss: 0.0661 - trial: 61142.9844 - score: 0.0655 - val_loss: 0.0917 - val_custom_loss: 0.0917 - val_trial: 61142.0000 - val_score: 0.0909 - lr: 0.0100\n",
            "Epoch 12/15\n",
            "1080/1080 [==============================] - ETA: 0s - loss: 0.0603 - custom_loss: 0.0603 - trial: 61142.9844 - score: 0.0598\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "1080/1080 [==============================] - 15s 13ms/step - loss: 0.0603 - custom_loss: 0.0603 - trial: 61142.9844 - score: 0.0598 - val_loss: 0.0583 - val_custom_loss: 0.0583 - val_trial: 61142.0000 - val_score: 0.0578 - lr: 0.0100\n",
            "Epoch 13/15\n",
            "1080/1080 [==============================] - 14s 13ms/step - loss: 0.0450 - custom_loss: 0.0450 - trial: 61142.9844 - score: 0.0446 - val_loss: 0.0931 - val_custom_loss: 0.0931 - val_trial: 61142.0000 - val_score: 0.0923 - lr: 0.0050\n",
            "Epoch 14/15\n",
            "1080/1080 [==============================] - 15s 13ms/step - loss: 0.0432 - custom_loss: 0.0432 - trial: 61142.9844 - score: 0.0428 - val_loss: 0.0353 - val_custom_loss: 0.0353 - val_trial: 61142.0000 - val_score: 0.0350 - lr: 0.0050\n",
            "Epoch 15/15\n",
            "1080/1080 [==============================] - 15s 13ms/step - loss: 0.0406 - custom_loss: 0.0406 - trial: 61142.9844 - score: 0.0402 - val_loss: 0.0388 - val_custom_loss: 0.0388 - val_trial: 61142.0000 - val_score: 0.0384 - lr: 0.0050\n",
            "\n",
            " It took 3.99803014198939 minutes to train!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from qkeras.autoqkeras import AutoQKeras\n",
        "\n",
        "autoqk = AutoQKeras(model, metrics=[custom_loss], output_dir=\"autoq_cnn\", **run_config)\n",
        "autoqk.fit(train_dataset, validation_data=test_dataset, epochs=10)\n",
        "\n",
        "aqmodel = autoqk.get_best_model()\n",
        "print_qmodel_summary(aqmodel)   \n",
        "\n",
        "# Train for the full epochs\n",
        "callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
        "            ]  \n",
        "\n",
        "start = time.time()\n",
        "history = aqmodel.fit(train_dataset,\n",
        "                      validation_data=test_dataset,\n",
        "                      epochs = 15,\n",
        "                      callbacks = callbacks, \n",
        "                      verbose=1)     \n",
        "end = time.time()\n",
        "print('\\n It took {} minutes to train!\\n'.format( (end - start)/60.))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aqmodel.save('saved_models/aqmodel_v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrM57RFGDcmf",
        "outputId": "74874f3f-2668-4a55-b27b-20c8a02e10bc"
      },
      "id": "wrM57RFGDcmf",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/aqmodel_v1/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/aqmodel_v1/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qkeras.autoqkeras import AutoQKeras\n",
        "from qkeras.utils import _add_supported_quantized_objects\n",
        "co = {}\n",
        "_add_supported_quantized_objects(co)"
      ],
      "metadata": {
        "id": "Vacm8QTh6Gzq"
      },
      "id": "Vacm8QTh6Gzq",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fc2e5402",
      "metadata": {
        "id": "fc2e5402"
      },
      "source": [
        "### Prune the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "TXq8xV1rxtVE"
      },
      "id": "TXq8xV1rxtVE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "35fa6ee9",
      "metadata": {
        "scrolled": true,
        "id": "35fa6ee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc788cae-de3f-4ef6-d626-51e8a0749fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:218: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  aggregation=tf.VariableAggregation.MEAN)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:225: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  aggregation=tf.VariableAggregation.MEAN)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:238: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  trainable=False)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training steps per epoch is 972\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
        "\n",
        "NSTEPS = int(data_train.shape[0]*0.9)  // BATCH_SIZE\n",
        "print('Number of training steps per epoch is {}'.format(NSTEPS))\n",
        "\n",
        "# Prune all convolutional and dense layers gradually from 0 to 50% sparsity every 2 epochs, \n",
        "# ending by the 10th epoch\n",
        "def pruneFunction(layer):\n",
        "    pruning_params = {'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity = 0.0,\n",
        "                                                                   final_sparsity = 0.50, \n",
        "                                                                   begin_step = NSTEPS*2, \n",
        "                                                                   end_step = NSTEPS*10, \n",
        "                                                                   frequency = NSTEPS)\n",
        "                     }\n",
        "    if isinstance(layer, tf.keras.layers.Conv1D):\n",
        "        return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "    if isinstance(layer, tf.keras.layers.Dense) and layer.name!='output_dense':\n",
        "        return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)  \n",
        "    return layer\n",
        "\n",
        "model_pruned = tf.keras.models.clone_model(aqmodel, clone_function=pruneFunction)\n",
        "\n",
        "# model_pruned = tfmot.sparsity.keras.prune_low_magnitude(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ab64d56e",
      "metadata": {
        "id": "ab64d56e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb878b7-ae4a-4e4c-ffc9-92879f9c4bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"SHO_Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 80, 2)]      0           []                               \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d (Pr  (None, 80, 16)      466         ['input_1[0][0]']                \n",
            " uneLowMagnitude)                                                                                 \n",
            "                                                                                                  \n",
            " activation (QActivation)       (None, 80, 16)       0           ['prune_low_magnitude_conv1d[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_1 (  (None, 80, 12)      2702        ['activation[1][0]']             \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_1 (QActivation)     (None, 80, 12)       0           ['prune_low_magnitude_conv1d_1[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_2 (  (None, 80, 6)       728         ['activation_1[1][0]']           \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_2 (QActivation)     (None, 80, 6)        0           ['prune_low_magnitude_conv1d_2[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 40, 6)        0           ['activation_2[1][0]']           \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_3 (  (None, 40, 1)       63          ['max_pooling1d[1][0]']          \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_3 (QActivation)     (None, 40, 1)        0           ['prune_low_magnitude_conv1d_3[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_4 (  (None, 40, 2)       24          ['activation_3[1][0]']           \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_4 (QActivation)     (None, 40, 2)        0           ['prune_low_magnitude_conv1d_4[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_5 (  (None, 40, 6)       128         ['activation_4[1][0]']           \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_5 (QActivation)     (None, 40, 6)        0           ['prune_low_magnitude_conv1d_5[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_6 (  (None, 40, 4)       246         ['activation_5[1][0]']           \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_6 (QActivation)     (None, 40, 4)        0           ['prune_low_magnitude_conv1d_6[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_7 (  (None, 40, 2)       84          ['activation_6[1][0]']           \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_7 (QActivation)     (None, 40, 2)        0           ['prune_low_magnitude_conv1d_7[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_8 (  (None, 40, 3)       65          ['activation_7[1][0]']           \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_8 (QActivation)     (None, 40, 3)        0           ['prune_low_magnitude_conv1d_8[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 20, 3)       0           ['activation_8[1][0]']           \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_9 (  (None, 20, 3)       59          ['average_pooling1d[1][0]']      \n",
            " PruneLowMagnitude)                                                                               \n",
            "                                                                                                  \n",
            " activation_9 (QActivation)     (None, 20, 3)        0           ['prune_low_magnitude_conv1d_9[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " average_pooling1d_1 (AveragePo  (None, 10, 3)       0           ['activation_9[1][0]']           \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " prune_low_magnitude_conv1d_10   (None, 10, 8)       154         ['average_pooling1d_1[1][0]']    \n",
            " (PruneLowMagnitude)                                                                              \n",
            "                                                                                                  \n",
            " activation_10 (QActivation)    (None, 10, 8)        0           ['prune_low_magnitude_conv1d_10[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 480)          0           ['activation_2[1][0]']           \n",
            "                                                                                                  \n",
            " average_pooling1d_2 (AveragePo  (None, 5, 8)        0           ['activation_10[1][0]']          \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " prune_low_magnitude_dense (Pru  (None, 10)          9612        ['flatten[1][0]']                \n",
            " neLowMagnitude)                                                                                  \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 40)           0           ['average_pooling1d_2[1][0]']    \n",
            "                                                                                                  \n",
            " prune_low_magnitude_dense_1 (P  (None, 40)          842         ['prune_low_magnitude_dense[0][0]\n",
            " runeLowMagnitude)                                               ']                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 80)           0           ['flatten_1[1][0]',              \n",
            "                                                                  'prune_low_magnitude_dense_1[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " prune_low_magnitude_dense_2 (P  (None, 12)          1934        ['concatenate[1][0]']            \n",
            " runeLowMagnitude)                                                                                \n",
            "                                                                                                  \n",
            " prune_low_magnitude_dense_3 (P  (None, 4)           102         ['prune_low_magnitude_dense_2[0][\n",
            " runeLowMagnitude)                                               0]']                             \n",
            "                                                                                                  \n",
            " prune_low_magnitude_dense_4 (P  (None, 4)           38          ['prune_low_magnitude_dense_3[0][\n",
            " runeLowMagnitude)                                               0]']                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 17,247\n",
            "Trainable params: 8,674\n",
            "Non-trainable params: 8,573\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_pruned.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "edd10292",
      "metadata": {
        "id": "edd10292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc24739d-a822-4300-c9a9-2179b0e55bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1080/1080 [==============================] - 30s 22ms/step - loss: 0.0515 - val_loss: 0.0596 - lr: 0.0050\n",
            "Epoch 2/10\n",
            "1080/1080 [==============================] - 22s 21ms/step - loss: 0.0477 - val_loss: 0.0526 - lr: 0.0050\n",
            "Epoch 3/10\n",
            "1080/1080 [==============================] - 22s 20ms/step - loss: 0.0414 - val_loss: 0.0360 - lr: 0.0050\n",
            "Epoch 4/10\n",
            "1080/1080 [==============================] - 22s 20ms/step - loss: 0.0545 - val_loss: 0.0418 - lr: 0.0050\n",
            "Epoch 5/10\n",
            "1080/1080 [==============================] - 22s 21ms/step - loss: 0.0547 - val_loss: 0.0518 - lr: 0.0050\n",
            "Epoch 6/10\n",
            "1078/1080 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "1080/1080 [==============================] - 22s 21ms/step - loss: 0.0515 - val_loss: 0.0439 - lr: 0.0050\n",
            "Epoch 7/10\n",
            "1080/1080 [==============================] - 22s 21ms/step - loss: 0.0530 - val_loss: 0.0573 - lr: 0.0025\n",
            "Epoch 8/10\n",
            "1080/1080 [==============================] - 22s 21ms/step - loss: 0.0503 - val_loss: 0.0551 - lr: 0.0025\n",
            "Epoch 9/10\n",
            "1079/1080 [============================>.] - ETA: 0s - loss: 0.0523\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "1080/1080 [==============================] - 22s 20ms/step - loss: 0.0523 - val_loss: 0.0560 - lr: 0.0025\n",
            "Epoch 10/10\n",
            "1080/1080 [==============================] - 23s 21ms/step - loss: 0.0452 - val_loss: 0.0406 - lr: 0.0012\n",
            "It took 3.89088180065155 minutes to train Keras model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as conv1d_layer_call_fn, conv1d_layer_call_and_return_conditional_losses, conv1d_1_layer_call_fn, conv1d_1_layer_call_and_return_conditional_losses, conv1d_2_layer_call_fn while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/pruned_cnn_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/pruned_cnn_model/assets\n"
          ]
        }
      ],
      "source": [
        "train = True # True if you want to retrain, false if you want to load a previsously trained model\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "if train:\n",
        "    model_pruned.compile(loss=custom_loss, optimizer=sgd)\n",
        "\n",
        "    callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
        "            pruning_callbacks.UpdatePruningStep()\n",
        "            ] \n",
        "    \n",
        "    start = time.time()\n",
        "    model_pruned.fit(train_dataset, validation_data=test_dataset, epochs=n_epochs, callbacks=callbacks, batch_size=BATCH_SIZE)   \n",
        "    end = time.time()\n",
        "\n",
        "    print('It took {} minutes to train Keras model'.format( (end - start)/60.))\n",
        "    \n",
        "    model_pruned.save('pruned_cnn_model.h5')\n",
        "    model_pruned.save('saved_models/pruned_cnn_model')\n",
        "\n",
        "else:\n",
        "    from qkeras.utils import _add_supported_quantized_objects\n",
        "    from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
        "    \n",
        "    co = {}\n",
        "    _add_supported_quantized_objects(co)\n",
        "    co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
        "    model_pruned = tf.keras.models.load_model('pruned_cnn_model.h5', custom_objects=co)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check sparsity"
      ],
      "metadata": {
        "id": "CbRNl5685dI3"
      },
      "id": "CbRNl5685dI3"
    },
    {
      "cell_type": "code",
      "source": [
        "def doWeights(model):\n",
        "\n",
        "    allWeightsByLayer = {}\n",
        "    for layer in model.layers:\n",
        "        if (layer._name).find(\"batch\")!=-1 or len(layer.get_weights())<1:\n",
        "            continue \n",
        "        weights=layer.weights[0].numpy().flatten()  \n",
        "        allWeightsByLayer[layer._name] = weights\n",
        "        print('Layer {}: % of zeros = {}'.format(layer._name,np.sum(weights==0)/np.size(weights)))\n",
        "\n",
        "    labelsW = []\n",
        "    histosW = []\n",
        "\n",
        "    for key in reversed(sorted(allWeightsByLayer.keys())):\n",
        "        labelsW.append(key)\n",
        "        histosW.append(allWeightsByLayer[key])\n",
        "\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    bins = np.linspace(-1.5, 1.5, 50)\n",
        "    plt.hist(histosW,bins,histtype='stepfilled',stacked=True,label=labelsW, edgecolor='black')\n",
        "    plt.legend(frameon=False,loc='upper left')\n",
        "    plt.ylabel('Number of Weights')\n",
        "    plt.xlabel('Weights')\n",
        "    plt.figtext(0.2, 0.38,model._name, wrap=True, horizontalalignment='left',verticalalignment='center')\n",
        "    \n",
        "doWeights(model_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8PFyKgft5daO",
        "outputId": "4a693d33-5189-483a-90d1-618e71cfb468"
      },
      "id": "8PFyKgft5daO",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return asarray(a).size\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer prune_low_magnitude_conv1d: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_1: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_2: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_3: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_4: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_5: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_6: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_7: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_8: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_conv1d_9: % of zeros = 0.48148148148148145\n",
            "Layer prune_low_magnitude_conv1d_10: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_dense: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_dense_1: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_dense_2: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_dense_3: % of zeros = 0.5\n",
            "Layer prune_low_magnitude_dense_4: % of zeros = 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff3f78c",
      "metadata": {
        "id": "aff3f78c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "Quantized Model v2.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}